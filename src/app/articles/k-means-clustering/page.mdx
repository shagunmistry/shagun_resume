import { ArticleLayout } from '@/components/ArticleLayout'
import { Card } from '@/components/Card'

export const article = {
  author: 'Shagun Mistry',
  date: '2024-11-24',
  title: 'K-Means Clustering',
  description:
    'K-Means Clustering is a popular unsupervised machine learning algorithm used for clustering data points into groups based on their similarity.',
}

export const metadata = {
  title: article.title,
  description: article.description,
}

export default (props) => <ArticleLayout article={article} {...props} />

## Prerequisites

Before diving into k-means clustering, ensure you have:

- Intermediate Python programming skills
- Basic understanding of NumPy and pandas
- Familiarity with basic statistical concepts
- Python environment with scikit-learn installed

### Environment Setup

Run the following command to set up your environment:

```bash
pip install numpy pandas scikit-learn matplotlib seaborn
```

## Introduction

K-means clustering is one of the most fundamental and widely-used unsupervised machine learning algorithms. It's particularly valuable in scenarios where you need to:

- Segment customer bases for targeted marketing
- Group similar documents or articles
- Identify patterns in geographic data
- Perform image compression through color quantization

## Core Concept: How K-Means Works

K-means clustering follows an iterative process to group data points into 'k' distinct clusters.

1. **Initialize Centroids**: Randomly place 'k' centroids in your feature space
2. **Assign Points**: Assign each data point to the nearest centroid using Euclidean distance
3. **Update Centroids**: Recalculate centroid positions based on the mean of all points in each cluster
4. **Iterate**: Repeat steps 2-3 until convergence (minimal centroid movement)

## Implementation

### Basic Implementation

Let's implement k-means clustering using scikit-learn with a simple example:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# Generate synthetic data
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

# Initialize and fit KMeans
kmeans = KMeans(n_clusters=4, random_state=0)
cluster_labels = kmeans.fit_predict(X)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', s=200, linewidths=3, color='red', label='Centroids')
plt.title('K-Means Clustering Results')
plt.legend()
plt.show()
```

## Best Practices and Common Pitfalls

### Guidelines

#### Do's

- Scale your features before clustering
- Use the elbow method to find optimal k
- Validate results using silhouette analysis
- Consider multiple random initializations

#### Don'ts

- Don't assume clusters are spherical
- Don't skip data preprocessing
- Don't rely solely on visual inspection
- Don't forget to handle outliers

## Practical Applications

### Customer Segmentation Example

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Sample customer data
customer_data = pd.DataFrame({
    'annual_income': [30000, 45000, 60000, 120000, 250000],
    'spending_score': [15, 35, 55, 75, 95]
})

# Preprocess data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(customer_data)

# Apply k-means
kmeans = KMeans(n_clusters=3, random_state=0)
customer_segments = kmeans.fit_predict(scaled_data)

# Add segments to dataframe
customer_data['Segment'] = customer_segments
```

## Advanced Topics

### Advanced Concepts

#### Variations of K-Means

- Mini-batch K-means for large datasets
- K-means++ for better initialization
- Soft K-means (fuzzy clustering)
- Kernel K-means for non-linear separation

#### Performance Metrics

- Silhouette Score
- Calinski-Harabasz Index
- Davies-Bouldin Index

## Next Steps

1. **Explore Other Clustering Algorithms**
   - DBSCAN for density-based clustering
   - Hierarchical clustering
   - Gaussian Mixture Models
2. **Practice with Real Datasets**
   - UCI Machine Learning Repository
   - Kaggle datasets
   - Your own domain-specific data
3. **Master Advanced Concepts**
   - Cluster validation techniques
   - Dimensionality reduction with PCA
   - Ensemble clustering methods

## Resources

### Learning Resources

- [Scikit-learn Documentation](https://scikit-learn.org/stable/modules/clustering.html)
- [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)
- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)
